{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import importlib \n",
    "import preprocessing\n",
    "importlib.reload(preprocessing)\n",
    "from preprocessing import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = import_data()\n",
    "X_train,X_test,y_train,y_test = train_test_total(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(features, labels, mode, params):\n",
    "  \n",
    "  \n",
    "  num_hidden_layers = len(params['hidden_layers'])  \n",
    "  \n",
    "  outputs = features[\"x\"]  \n",
    "\n",
    "  model = params['model']\n",
    "    \n",
    "  activation = params.get(\"activation\", tf.nn.tanh)\n",
    "\n",
    "  print(\"Using activation - \", activation)\n",
    "\n",
    "  for i in range(num_hidden_layers):  \n",
    "\n",
    "    \n",
    "      cell = model(name = 'cell'+str(i), num_units = params['hidden_layers'][i])\n",
    "      outputs, state = tf.nn.dynamic_rnn(cell=cell,\n",
    "                                       inputs=outputs,\n",
    "                                       dtype=tf.float64)\n",
    "    \n",
    "      #outputs = tf.layers.batch_normalization(outputs)\n",
    "        \n",
    "      outputs = activation(outputs)\n",
    "      outputs = tf.nn.dropout(outputs, params['dropout'])\n",
    "\n",
    "  #FLatten the output of LSTM layers\n",
    "  outputs = tf.contrib.layers.flatten(outputs) \n",
    "\n",
    "  # FC Layer\n",
    "  logits = tf.layers.dense(inputs=outputs, units=params['num_classes'])\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(eeg_classifier, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": X_train},\n",
    "        y=y_train,\n",
    "        batch_size=50,\n",
    "        num_epochs=20,\n",
    "        shuffle=True)\n",
    "\n",
    "    eeg_classifier.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=100)\n",
    "\n",
    "    eval_test_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": X_test},\n",
    "        y=y_test,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "    eval_train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": X_train},\n",
    "        y=y_train,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "    train_results = eeg_classifier.evaluate(input_fn=eval_train_fn)\n",
    "    test_results = eeg_classifier.evaluate(input_fn=eval_test_fn)\n",
    "    print(train_results)\n",
    "    print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [tf.nn.relu, tf.nn.crelu, tf.nn.elu, tf.nn.selu, tf.nn.softplus, tf.nn.softsign,\n",
    "               tf.sigmoid, tf.tanh]\n",
    "\n",
    "for act in activations:\n",
    "    \n",
    "    print(\"=\"*75)\n",
    "    print('Trying out activation - ', str(act))\n",
    "    \n",
    "    eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n",
    "                                            params = {'hidden_layers' : [64, 64], 'num_classes' : 4, \n",
    "                                                      'learning_rate' : 0.001, 'model' : tf.nn.rnn_cell.LSTMCell,\n",
    "                                                     'dropout' : 0.5, 'activation' : act})\n",
    "\n",
    "\n",
    "    calc(eeg_classifier, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
